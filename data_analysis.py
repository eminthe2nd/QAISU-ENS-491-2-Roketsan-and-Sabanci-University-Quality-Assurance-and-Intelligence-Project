# -*- coding: utf-8 -*-
"""data_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVdDjctoYX2ctLIR_f_H4m3vMhiAbQOG

### DATA ANALYSIS - PHASE 1 (EXPLORATORY ANALYSIS)

This file was prepared for exploratory data analysis (EDA) on the 'Fixed_Random_Synthetic1.xlsx' data set.

Purpose:
- To analyze the data structure
- To check for missing values
- To obtain summary statistics for numerical and categorical variables
- To report preliminary findings with visuals and commentary

**1.1 LOADING DATA AND INITIAL EXAMINATION**

We load the Excel file into the Pandas DataFrame. Data size, number of columns, data types, and missing value structures can be seen.
"""

import pandas as pd

df = pd.read_excel("Sabitlenmiş_Random_Sentetik1.xlsx")
df.head()

print("Number of rows:", df.shape[0])
print("Number of columns:", df.shape[1])
print("\nData Types:\n", df.dtypes.value_counts())
df.info()

missing = df.isnull().sum().sort_values(ascending=False)
missing_percent = (missing / len(df)) * 100
missing_table = pd.DataFrame({"Missing count": missing, "Missing (%)": missing_percent})
missing_table.head(10)

"""**1.2 EXPLORING NUMERICAL VARIABLES**

Viewing statistics such as mean, standard deviation, minimum, and maximum aimed for this part. Only numeric columns (int, float) are analyzed in this section.
Categorical columns are not included in this step.
"""

df.describe()

"""**1.2.1 Histograms and Insights**

- The dataset is largely clean, with no missing values in numeric columns.  
- Most numeric variables are uniformly or randomly distributed, consistent with a synthetic generation process.  
- A subset of variables (`NUMUNE_MIKTARI`, `RET_MIKTAR`, `YARATILMA_TARIHI`) demonstrate right-skewed patterns, indicating that most values are small but a few observations have very large values, potentially requiring log or square-root transformations for normalization.  
- Some variables such as `MM` follow an **approximately normal distribution**, which is ideal for modeling.
- Variables representing identifiers (`PROJE_NO`, `IS_EMRI`, `SIRA_NO`) and limits (`ALT_LIMIT`, `UST_LIMIT`) should be excluded from correlation or regression analyses.  
- Variables representing limits (`ALT_LIMIT`, `UST_LIMIT`) or dates (`PRT_START_DATE`, `PRT_COMPLETE_DATE`) have **specific ranges** and should be interpreted contextually rather than statistically.
- Future modeling should consider standardizing skewed variables to ensure consistent scale and variance across predictors.

"""

import matplotlib.pyplot as plt

df.hist(figsize=(15,10), bins=20)
plt.tight_layout()
plt.show()

"""**1.2.2 Correlation Analysis**

The correlation heatmap reveals how strongly numeric variables are linearly related to each other.

- Most variables show **weak or no linear correlation**, which is expected for a synthetically generated dataset.
- However, a few variable pairs exhibit **moderate to strong positive relationships**:
  - `NUMUNE_MIKTARI` and `RET_MIKTAR` (r ≈ 0.72)
  - `NUMUNE_MIKTARI` and `ALIKONULAN_MIKTAR` (r ≈ 0.62)
  - `PRT_START_DATE` and `PRT_COMPLETE_DATE` (r ≈ 0.67)
  - `YARATILMA_TARIHI` and `PRT_START_DATE` (r ≈ 0.68)
- These suggest that variables related to material quantity and process timing evolve together.
- No meaningful negative correlations were observed.

Interpretation:
Variables that represent quantities (sample, retained, and rejected amounts) are naturally interdependent.  
Date-related variables also correlate because they occur sequentially in process flow.  
All other numeric features appear independent, indicating a well-separated data structure for analysis.

"""

corr = df.corr(numeric_only=True)
corr.style.background_gradient(cmap="coolwarm").format(precision=2)

"""**1.3 DISCOVERING CATEGORICAL VARIABLES**

Determining the most frequent values, the number of unique values, and their frequencies are the purposes of this part. This analysis is applied to columns of type 'object'. This way, we can understand which values ​​occur more frequently in variables such as 'ERROR_TYPE' and 'CONFIRM_STATUS'.

"""

object_cols = df.select_dtypes(include='object').columns


categorical_summary = df[object_cols].describe()


categorical_summary_tidy = categorical_summary.T


categorical_summary_tidy = categorical_summary_tidy.rename(columns={
    "count": "Count",
    "unique": "Unique",
    "top": "Top",
    "freq": "Freq"
})


from IPython.display import display
display(categorical_summary_tidy.head(30))

"""-The distribution of categorical variables indicates that approximately 30% of the dataset consists of high-cardinality variables (e.g., IS_EMRI_TAMAMLANMA_STOK_YERI), 40% exhibit moderate diversity, and the remaining variables contain only a few distinct categories.

-Variables related to error classification (e.g., HATA_TURU: 10 categories, HATA_SINIFI: 75 categories) demonstrate a detailed labeling structure, suggesting that the data generation process included comprehensive process-level categorization.

-Low-cardinality variables (e.g., KRITER, ONAY_STATUSU, Rapor Kapsamı) may serve as potential target variables in supervised learning scenarios.

-Some categorical variables (e.g., OPERASYON_ADI, TASARIM_SORUMLUSU) capture operational diversity within the process and can be used for group-based performance analysis or defect-rate comparisons.

### DATA ANALYSIS - PHASE 2
"""

print("\n---Correlation Analysis ---")
corr_matrix = df.corr(numeric_only=True)


corr_pairs = corr_matrix.unstack().dropna()
corr_pairs = corr_pairs[corr_pairs != 1]
sorted_corr = corr_pairs.sort_values(ascending=False)

top3_pos = sorted_corr.head(3)
top3_neg = sorted_corr.tail(3)

print("\nThe 3 highest positive correlations:")
print(top3_pos)

print("\nThe 3 highest negative correlations:")
print(top3_neg)

# --- Outlier Analysis ---
outlier_summary = {}

for col in df.select_dtypes(include=['float64', 'int64']).columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower) | (df[col] > upper)]
    outlier_summary[col] = len(outliers) / len(df) * 100

outlier_df = pd.DataFrame.from_dict(outlier_summary, orient='index', columns=['Outlier (%)'])
outlier_df.sort_values(by='Outlier (%)', ascending=False, inplace=True)

print("\n--- utlier Rates ---")
print(outlier_df.head(10))

# --- Analiz 1 ve Analiz 2 Hesaplamaları ---

# Analiz 1: Ölçümün alt ve üst limitlere göre normalize sapması
# 0 ile 1 arası değerler tolerans içinde; <0 alt limitin altında, >1 üst limitin üstünde demektir.
df["Analiz 1"] = (df["NUMERIK_SONUC"] - df["ALT_LIMIT"]) / (df["UST_LIMIT"] - df["ALT_LIMIT"])

# Analiz 2: Red miktarının numune miktarına oranı (kalite göstergesi)
df["Analiz 2"] = df["RET_MIKTAR"] / df["NUMUNE_MIKTARI"]

# Sonuç kontrolü
print(df[["Analiz 1", "Analiz 2"]].describe())

#Dosyayı kaydet
df.to_excel("Sabitlenmiş_Random_Sentetik1_Analizli.xlsx", index=False)